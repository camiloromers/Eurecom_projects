{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align:center\"> Convolutional Neural Network (CNN) for Handwritten Digits Recognition</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Student 1:</b> Camilo Gabriel ROMERO SANCHEZ \n",
    "<b> Student 2:</b> Daniel HOMS ALFOSIN  \n",
    "<b> Group name:</b> deeplearn6\n",
    "  \n",
    " \n",
    "The aim of this session is to practice with Convolutional Neural Networks. Each group should fill and run appropriate notebook cells. \n",
    "\n",
    "\n",
    "Generate your final report (export as HTML) and upload it on the submission website http://bigfoot-m1.eurecom.fr/teachingsub/login (using your deeplearnXX/password). Do not forget to run all your cells before generating your final report and do not forget to include the names of all participants in the group. The lab session should be completed and submitted by May 30th 2018 (23:59:59 CET)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous Lab Session, you built a Multilayer Perceptron for recognizing hand-written digits from the MNIST data-set. The best achieved accuracy on testing data was about 97%. Can you do better than these results using a deep CNN ?\n",
    "In this Lab Session, you will build, train and optimize in TensorFlow one of the early Convolutional Neural Networks,  **LeNet-5**, to go to more than 99% of accuracy. \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load MNIST Data in TensorFlow\n",
    "Run the cell below to load the MNIST data that comes with TensorFlow. You will use this data in **Section 1** and **Section 2**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "Image Shape: (784,)\n",
      "Training Set:   55000 samples\n",
      "Validation Set: 5000 samples\n",
      "Test Set:       10000 samples\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "from datetime import timedelta\n",
    "\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "X_train, y_train           = mnist.train.images, mnist.train.labels\n",
    "X_validation, y_validation = mnist.validation.images, mnist.validation.labels\n",
    "X_test, y_test             = mnist.test.images, mnist.test.labels\n",
    "print(\"Image Shape: {}\".format(X_train[0].shape))\n",
    "print(\"Training Set:   {} samples\".format(len(X_train)))\n",
    "print(\"Validation Set: {} samples\".format(len(X_validation)))\n",
    "print(\"Test Set:       {} samples\".format(len(X_test)))\n",
    "\n",
    "epsilon = 1e-10 # this is a parameter you will use later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1 : My First Model in TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before starting with CNN, let's train and test in TensorFlow the example\n",
    "**y=softmax(Wx+b)** seen in the first lab. \n",
    "\n",
    "This model reaches an accuracy of about 92 %.\n",
    "You will also learn how to launch the TensorBoard https://www.tensorflow.org/get_started/summaries_and_tensorboard to visualize the computation graph, statistics and learning curves. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Part 1 </b> : Read carefully the code in the cell below. Run it to perform training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  01   =====> Loss= 1.288146603\n",
      "Epoch:  02   =====> Loss= 0.732879841\n",
      "Epoch:  03   =====> Loss= 0.600437544\n",
      "Epoch:  04   =====> Loss= 0.536478129\n",
      "Epoch:  05   =====> Loss= 0.497883815\n",
      "Epoch:  06   =====> Loss= 0.471082701\n",
      "Epoch:  07   =====> Loss= 0.451253696\n",
      "Epoch:  08   =====> Loss= 0.436073195\n",
      "Epoch:  09   =====> Loss= 0.423407513\n",
      "Epoch:  10   =====> Loss= 0.413047058\n",
      "Epoch:  11   =====> Loss= 0.404533125\n",
      "Epoch:  12   =====> Loss= 0.396643906\n",
      "Epoch:  13   =====> Loss= 0.390263800\n",
      "Epoch:  14   =====> Loss= 0.384461008\n",
      "Epoch:  15   =====> Loss= 0.379242165\n",
      "Epoch:  16   =====> Loss= 0.374706218\n",
      "Epoch:  17   =====> Loss= 0.370245252\n",
      "Epoch:  18   =====> Loss= 0.366504482\n",
      "Epoch:  19   =====> Loss= 0.362957360\n",
      "Epoch:  20   =====> Loss= 0.359751847\n",
      "Epoch:  21   =====> Loss= 0.356848331\n",
      "Epoch:  22   =====> Loss= 0.353783526\n",
      "Epoch:  23   =====> Loss= 0.351158460\n",
      "Epoch:  24   =====> Loss= 0.348740204\n",
      "Epoch:  25   =====> Loss= 0.346141227\n",
      "Epoch:  26   =====> Loss= 0.344057238\n",
      "Epoch:  27   =====> Loss= 0.342394533\n",
      "Epoch:  28   =====> Loss= 0.340264676\n",
      "Epoch:  29   =====> Loss= 0.338587863\n",
      "Epoch:  30   =====> Loss= 0.336686332\n",
      "Epoch:  31   =====> Loss= 0.334882768\n",
      "Epoch:  32   =====> Loss= 0.333570118\n",
      "Epoch:  33   =====> Loss= 0.331723409\n",
      "Epoch:  34   =====> Loss= 0.330510416\n",
      "Epoch:  35   =====> Loss= 0.329218664\n",
      "Epoch:  36   =====> Loss= 0.327934043\n",
      "Epoch:  37   =====> Loss= 0.326688678\n",
      "Epoch:  38   =====> Loss= 0.325190353\n",
      "Epoch:  39   =====> Loss= 0.324209239\n",
      "Epoch:  40   =====> Loss= 0.323259278\n",
      "Optimization Finished!\n",
      "Accuracy: 0.9164\n"
     ]
    }
   ],
   "source": [
    "#STEP 1\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.01\n",
    "training_epochs = 40\n",
    "batch_size = 128\n",
    "display_step = 1\n",
    "logs_path = 'log_files/'  # useful for tensorboard\n",
    "\n",
    "# tf Graph Input:  mnist data image of shape 28*28=784\n",
    "x = tf.placeholder(tf.float32, [None, 784], name='InputData')\n",
    "# 0-9 digits recognition,  10 classes\n",
    "y = tf.placeholder(tf.float32, [None, 10], name='LabelData')\n",
    "\n",
    "# Set model weights\n",
    "W = tf.Variable(tf.zeros([784, 10]), name='Weights')\n",
    "b = tf.Variable(tf.zeros([10]), name='Bias')\n",
    "\n",
    "# Construct model and encapsulating all ops into scopes, making Tensorboard's Graph visualization more convenient\n",
    "with tf.name_scope('Model'):\n",
    "    # Model\n",
    "    pred = tf.nn.softmax(tf.matmul(x, W) + b) # Softmax\n",
    "with tf.name_scope('Loss'):\n",
    "    # Minimize error using cross entropy\n",
    "    # We use tf.clip_by_value to avoid having too low numbers in the log function\n",
    "    cost = tf.reduce_mean(-tf.reduce_sum(y*tf.log(tf.clip_by_value(pred, epsilon, 1.0)), reduction_indices=1))\n",
    "with tf.name_scope('SGD'):\n",
    "    # Gradient Descent\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)\n",
    "with tf.name_scope('Accuracy'):\n",
    "    # Accuracy\n",
    "    acc = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "    acc = tf.reduce_mean(tf.cast(acc, tf.float32))\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()\n",
    "# Create a summary to monitor cost tensor\n",
    "tf.summary.scalar(\"Loss\", cost)\n",
    "# Create a summary to monitor accuracy tensor\n",
    "tf.summary.scalar(\"Accuracy\", acc)\n",
    "# Merge all summaries into a single op\n",
    "merged_summary_op = tf.summary.merge_all()\n",
    "\n",
    "#STEP 2 \n",
    "\n",
    "# Launch the graph for training\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    # op to write logs to Tensorboard\n",
    "    summary_writer = tf.summary.FileWriter(logs_path, graph=tf.get_default_graph())\n",
    "    # Training cycle\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0.\n",
    "        total_batch = int(mnist.train.num_examples/batch_size)\n",
    "        # Loop over all batches\n",
    "        for i in range(total_batch):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size, shuffle=(i==0))\n",
    "            # Run optimization op (backprop), cost op (to get loss value)\n",
    "            # and summary nodes\n",
    "            _, c, summary = sess.run([optimizer, cost, merged_summary_op],\n",
    "                                     feed_dict={x: batch_xs, y: batch_ys})\n",
    "            # Write logs at every iteration\n",
    "            summary_writer.add_summary(summary, epoch * total_batch + i)\n",
    "            # Compute average loss\n",
    "            avg_cost += c / total_batch\n",
    "        # Display logs per epoch step\n",
    "        if (epoch+1) % display_step == 0:\n",
    "            print(\"Epoch: \", '%02d' % (epoch+1), \"  =====> Loss=\", \"{:.9f}\".format(avg_cost))\n",
    "\n",
    "    print(\"Optimization Finished!\")\n",
    "    summary_writer.flush()\n",
    "\n",
    "    # Test model\n",
    "    # Calculate accuracy\n",
    "    print(\"Accuracy:\", acc.eval({x: mnist.test.images, y: mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Part 2  </b>: Using Tensorboard, we can  now visualize the created graph, giving you an overview of your architecture and how all of the major components  are connected. You can also see and analyse the learning curves. \n",
    "\n",
    "To launch tensorBoard: \n",
    "- Open a Terminal and run the command line **\"tensorboard --logdir=lab_2/log_files/\"**\n",
    "- Click on \"Tensorboard web interface\" in Zoe  \n",
    "\n",
    "\n",
    "Enjoy It !! \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 2 : The 99% MNIST Challenge !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Part 1 </b> : LeNet5 implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are now more familar with **TensorFlow** and **TensorBoard**. In this section, you are to build, train and test the baseline [LeNet-5](http://yann.lecun.com/exdb/lenet/)  model for the MNIST digits recognition problem.  \n",
    "\n",
    "Then, you will make some optimizations to get more than 99% of accuracy.\n",
    "\n",
    "For more informations, have a look at this list of results: http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"lenet.png\",width=\"800\" height=\"600\" align=\"center\">\n",
    "<center><span>Figure 1: Lenet-5 </span></center>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "The LeNet architecture takes a 28x28xC image as input, where C is the number of color channels. Since MNIST images are grayscale, C is 1 in this case.\n",
    "\n",
    "--------------------------\n",
    "**Layer 1 - Convolution (5x5):** The output shape should be 28x28x6. **Activation:** ReLU. **MaxPooling:** The output shape should be 14x14x6.\n",
    "\n",
    "**Layer 2 - Convolution (5x5):** The output shape should be 10x10x16. **Activation:** ReLU. **MaxPooling:** The output shape should be 5x5x16.\n",
    "\n",
    "**Flatten:** Flatten the output shape of the final pooling layer such that it's 1D instead of 3D.  You may need to use tf.reshape.\n",
    "\n",
    "**Layer 3 - Fully Connected:** This should have 120 outputs. **Activation:** ReLU.\n",
    "\n",
    "**Layer 4 - Fully Connected:** This should have 84 outputs. **Activation:** ReLU.\n",
    "\n",
    "**Layer 5 - Fully Connected:** This should have 10 outputs. **Activation:** softmax.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Question 2.1.1 </b>  Implement the Neural Network architecture described above.\n",
    "For that, your will use classes and functions from  https://www.tensorflow.org/api_docs/python/tf/nn. \n",
    "\n",
    "We give you some helper functions for weigths and bias initilization. Also you can refer to section 1. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions for weigths and bias initilization \n",
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0., shape=[shape])\n",
    "    return tf.Variable(initial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Definition parameters of CNN'''\n",
    "\n",
    "# Conv layer 1\n",
    "filter_size_1 = 5\n",
    "num_filters_1 = 6\n",
    "padding_1 = 'SAME'\n",
    "\n",
    "# Conv layer 2\n",
    "filter_size_2 = 5\n",
    "num_filters_2 = 16\n",
    "padding_2 = 'VALID'\n",
    "\n",
    "# Conv layer 3\n",
    "fc_size_1 = 120\n",
    "\n",
    "# Conv layer 4\n",
    "fc_size_2 = 84"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNIST images size in pixel.\n",
    "img_size = 28\n",
    "\n",
    "# One-dimensional arrays of images.\n",
    "img_size_flat = img_size * img_size\n",
    "img_shape = (img_size, img_size)\n",
    "\n",
    "# Colour channels (grey-scale).\n",
    "num_channels = 1\n",
    "\n",
    "# Number of classes equal to 10 digits.\n",
    "num_classes = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LeNet5_Model(input,                # Input layer.\n",
    "                 num_input_channels,   # Channels in previous layer.\n",
    "                 filter_size,          # Width and height filter.\n",
    "                 num_filters,          # Number of filters.\n",
    "                 padding,              # Padding approach.\n",
    "                 activ_pooling=True):  # Use 2x2 max-pooling.\n",
    "\n",
    "    # Shape of the filter-weights.\n",
    "    shape = [filter_size, filter_size, num_input_channels, num_filters]\n",
    "\n",
    "    # Initialize weight given shape.\n",
    "    weights = weight_variable(shape=shape)\n",
    "\n",
    "    # Initialize bias given shape, one for each filter.\n",
    "    biases = bias_variable(shape=num_filters)\n",
    "\n",
    "    # Convolution operation.\n",
    "    layer = tf.nn.conv2d(input=input,\n",
    "                         filter=weights,\n",
    "                         strides=[1, 1, 1, 1],\n",
    "                         padding=padding)\n",
    "\n",
    "    # Add the biases to the results of the convolution.\n",
    "    # A bias-value is added to each filter-channel.\n",
    "    layer += biases\n",
    "\n",
    "    # Use pooling according to LeNet-5 architecture.\n",
    "    if activ_pooling:\n",
    "        # This is 2x2 max-pooling, which applies\n",
    "        # in all cases of CNN architecture.\n",
    "        layer = tf.nn.max_pool(value=layer,\n",
    "                               ksize=[1, 2, 2, 1],\n",
    "                               strides=[1, 2, 2, 1],\n",
    "                               padding='VALID')\n",
    "\n",
    "    # Rectified Linear Unit (ReLU).\n",
    "    layer = tf.nn.relu(layer)\n",
    "\n",
    "    return layer, weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_layer(layer):\n",
    "    # Shape of the input layer.\n",
    "    layer_shape = layer.get_shape()\n",
    "\n",
    "    # Number of features\n",
    "    num_features = layer_shape[1:4].num_elements()\n",
    "    \n",
    "    # Reshape the layer to [num_images, num_features].\n",
    "    layer_flat = tf.reshape(layer, [-1, num_features])\n",
    "\n",
    "    return layer_flat, num_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_fc_layer(input,          # The previous layer.\n",
    "                 num_inputs,     # Num. inputs from prev. layer.\n",
    "                 num_outputs):   # Num. outputs.\n",
    "\n",
    "    # Create new weights and biases.\n",
    "    weights = weight_variable(shape=[num_inputs, num_outputs])\n",
    "    biases = bias_variable(shape=num_outputs)\n",
    "\n",
    "    # Calculate the layer as the matrix multiplication of\n",
    "    # the input and weights, and then add the bias-values.\n",
    "    layer = tf.matmul(input, weights) + biases\n",
    "    \n",
    "    # Passing ReLu for non linear operation.\n",
    "    layer = tf.nn.relu(layer)\n",
    "\n",
    "    return layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Question 2.1.2. </b>  Calculate the number of parameters of this model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Layer S1 **<br>\n",
    "In the first layer we have 5 size filer, so there are (filer_size x filer_size + bias) x #_filters <br>\n",
    "(5x5+1)*6 = 156 parameters to lean. <br>\n",
    "Assuming all connections, there will be: <br>\n",
    "Total connetions: 28x28x156 = 122304<br>\n",
    "\n",
    "** Layer S2 **<br>\n",
    "(6x2) = 12 parameters to lean. <br>\n",
    "\n",
    "** Layer S3 **<br>\n",
    "5x5x(6x3 + 9x4 +1x6) + 16 = 1516 parameters to lean. <br>\n",
    "\n",
    "** Layer S4 **<br>\n",
    "(16x2) = 32 parameters to lean. <br>\n",
    "\n",
    "** Layer C5 **<br>\n",
    "120x(16x25+1) =\t48120 parameters to lean. <br>\n",
    "\n",
    "** Layer F6 **<br>\n",
    "84x(120+1) = 10164 parameters to lean. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Question 2.1.3. </b>  Define your model, its accuracy and the loss function according to the following parameters (you can look at Section 1 to see what is expected):\n",
    "\n",
    "     Learning rate: 0.001\n",
    "     Loss Fucntion: Cross-entropy\n",
    "     Optimizer: tf.train.GradientDescentOptimizer\n",
    "     Number of epochs: 40\n",
    "     Batch size: 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph() # reset the default graph before defining a new model\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.01\n",
    "training_epochs = 40\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Input variables'''\n",
    "x = tf.placeholder(tf.float32, shape=[None, img_size_flat], name='images')\n",
    "y_true = tf.placeholder(tf.float32, shape=[None, num_classes], name='y_true')\n",
    "\n",
    "x_image = tf.reshape(x, [-1, img_size, img_size, num_channels])\n",
    "\n",
    "'''CNN Tensorflow architecture'''\n",
    "# Convolutional layer 2\n",
    "layer_conv1, weights_conv1 = LeNet5_Model(input=x_image,\n",
    "                                           num_input_channels=num_channels,\n",
    "                                           filter_size=filter_size_1,\n",
    "                                           num_filters=num_filters_1,\n",
    "                                           padding=padding_1,\n",
    "                                           activ_pooling=True)\n",
    "# Convolutional layer 2\n",
    "layer_conv2, weights_conv2 = LeNet5_Model(input=layer_conv1,\n",
    "                                           num_input_channels=num_filters_1,\n",
    "                                           filter_size=filter_size_2,\n",
    "                                           num_filters=num_filters_2,\n",
    "                                           padding=padding_2,\n",
    "                                           activ_pooling=True)\n",
    "\n",
    "# Flatten layer\n",
    "layer_flat, num_features = flatten_layer(layer_conv2)\n",
    "\n",
    "# Fully Connectes layer 1\n",
    "fc_layer_1 = new_fc_layer(input=layer_flat,\n",
    "                         num_inputs=num_features,\n",
    "                         num_outputs=fc_size_1)\n",
    "\n",
    "\n",
    "# Fully Connectes layer 2\n",
    "fc_layer_2 = new_fc_layer(input=fc_layer_1,\n",
    "                         num_inputs=fc_size_1,\n",
    "                         num_outputs=fc_size_2)\n",
    "\n",
    "\n",
    "# Fully Connectes layer 3\n",
    "fc_layer_3 = new_fc_layer(input=fc_layer_2,\n",
    "                         num_inputs=fc_size_2,\n",
    "                         num_outputs=num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Question 2.1.4. </b>  Implement the evaluation function for accuracy computation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction label from model (max value of Softmax operation).\n",
    "y_pred_cls = tf.argmax(tf.nn.softmax(fc_layer_3), axis=1)\n",
    "\n",
    "# Real label from data\n",
    "y_true_cls = tf.argmax(y_true, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(logits, labels):\n",
    "    # logits will be the outputs of your model, labels will be one-hot vectors corresponding to the actual labels\n",
    "    # logits and labels are numpy arrays\n",
    "    # this function should return the accuracy of your model\n",
    "    \n",
    "    # Measuring prediction between model and real data\n",
    "    difference_model_pred = tf.equal(logits, labels)\n",
    "    \n",
    "    # Return accurancy\n",
    "    return tf.reduce_mean(tf.cast(difference_model_pred, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = evaluate(y_pred_cls, y_true_cls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implement an optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function.\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits_v2(logits=fc_layer_3,\n",
    "                                                        labels=y_true)\n",
    "# Loss function.\n",
    "cost = tf.reduce_mean(cross_entropy)\n",
    "\n",
    "# Accuracy.\n",
    "accuracy = evaluate(y_pred_cls, y_true_cls)\n",
    "\n",
    "# Gradient Descent optimizer.\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Question 2.1.5. </b>  Implement training pipeline and run the training data through it to train the model.\n",
    "\n",
    "- Before each epoch, shuffle the training set. \n",
    "- Print the loss per mini batch and the training/validation accuracy per epoch. (Display results every 100 epochs)\n",
    "- Save the model after training\n",
    "- Print after training the final testing accuracy \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing the variables.\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Create a summary to monitor cost tensor.\n",
    "tf.summary.scalar('Cost_GD', cost)\n",
    "\n",
    "# # Create a summary to monitor accuracy tensor.\n",
    "# tf.summary.scalar('Accuracy_GD', accuracy)\n",
    "\n",
    "# Merge all summaries into a single op.\n",
    "merged_summary_op = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(init, sess, n_epochs, batch_size, optimizer, cost, merged_summary_op):\n",
    "    \n",
    "    # Start-time used for printing time-usage below.\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        \n",
    "        avg_cost = 0.\n",
    "        total_batch = int(mnist.train.num_examples/batch_size)\n",
    "        \n",
    "        # Loop over all batches\n",
    "        for i in range(total_batch):\n",
    "               \n",
    "            # Batch data for train_x and train_y\n",
    "            x_batch, y_true_batch = mnist.train.next_batch(batch_size, \n",
    "                                                           shuffle=(i==0))\n",
    "\n",
    "            # Feed train batch data as dic.\n",
    "            feed_train = {x: x_batch, \n",
    "                          y_true: y_true_batch}\n",
    "\n",
    "            # Run graph.\n",
    "            _, c, _, summary = sess.run([optimizer, cost, accuracy, merged_summary_op],\n",
    "                                          feed_dict=feed_train)\n",
    "            \n",
    "            # Compute average loss\n",
    "            avg_cost += c / total_batch\n",
    "            \n",
    "            # Write logs at every iteration\n",
    "            summary_writer.add_summary(summary, epoch * total_batch + i)\n",
    "            \n",
    "        # Display logs per epoch step\n",
    "        if (epoch+1) % display_step == 0:\n",
    "            acur = sess.run(accuracy, feed_dict=feed_train)\n",
    "            print(\"Epoch: \", '%02d' % (epoch+1), \" ===> Loss=\", \"{:.5f}\".format(avg_cost), \", Training acurracy=\", \"{:.2%}\".format(acur))\n",
    "   \n",
    "    # Ending time.\n",
    "    end_time = time.time()\n",
    "    \n",
    "    # Difference between start and end-times.\n",
    "    dif = end_time - start_time\n",
    "\n",
    "    # Print the time-usage.\n",
    "    print('\\nTime usage: ' + str(timedelta(seconds=int(round(dif)))))\n",
    "    \n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the test-set into smaller batches of this size.\n",
    "test_batch_size = 256\n",
    "\n",
    "def test_accuracy():\n",
    "\n",
    "    # Number of images in the test-set.\n",
    "    num_test = len(mnist.test.images)\n",
    "\n",
    "    # Allocate an array for the predicted classes which\n",
    "    # will be calculated in batches and filled into this array.\n",
    "    cls_pred = np.zeros(shape=num_test, dtype=np.int)\n",
    "\n",
    "    i = 0\n",
    "\n",
    "    while i < num_test:\n",
    "        # The ending index for the next batch is denoted j.\n",
    "        j = min(i + test_batch_size, num_test)\n",
    "\n",
    "        # Get the images from the test-set between index i and j.\n",
    "        images = mnist.test.images[i:j, :]\n",
    "\n",
    "        # Get the associated labels.\n",
    "        labels = mnist.test.labels[i:j, :]\n",
    "\n",
    "        # Create a feed-dict with these images and labels.\n",
    "        feed_dict = {x: images,\n",
    "                     y_true: labels}\n",
    "\n",
    "        # Calculate the predicted class using TensorFlow.\n",
    "        cls_pred[i:j] = sess.run(y_pred_cls, feed_dict=feed_dict)\n",
    "\n",
    "        # Set the start-index for the next batch to the\n",
    "        # end-index of the current batch.\n",
    "        i = j\n",
    "\n",
    "    # Convenience variable for the true class-numbers of the test-set.\n",
    "    cls_true = np.argmax(mnist.test.labels, axis=1)\n",
    "\n",
    "    # Create a boolean array whether each image is correctly classified.\n",
    "    correct = (cls_true == cls_pred)\n",
    "\n",
    "    # Calculate the number of correctly classified images.\n",
    "    # When summing a boolean array, False means 0 and True means 1.\n",
    "    correct_sum = correct.sum()\n",
    "\n",
    "    # Classification accuracy is the number of correctly classified\n",
    "    # images divided by the total number of images in the test-set.\n",
    "    acc = float(correct_sum) / num_test\n",
    "\n",
    "    print('\\n\\nAccuracy Test Set: {0:.1%} ({1} / {2})'.format(acc, correct_sum, num_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  01  ===> Loss= 1.73335 , Training acurracy= 71.88%\n",
      "Epoch:  02  ===> Loss= 0.86090 , Training acurracy= 72.66%\n",
      "Epoch:  03  ===> Loss= 0.72863 , Training acurracy= 70.31%\n",
      "Epoch:  04  ===> Loss= 0.56721 , Training acurracy= 87.50%\n",
      "Epoch:  05  ===> Loss= 0.40811 , Training acurracy= 84.38%\n",
      "Epoch:  06  ===> Loss= 0.37747 , Training acurracy= 82.81%\n",
      "Epoch:  07  ===> Loss= 0.35909 , Training acurracy= 89.84%\n",
      "Epoch:  08  ===> Loss= 0.14375 , Training acurracy= 97.66%\n",
      "Epoch:  09  ===> Loss= 0.11789 , Training acurracy= 99.22%\n",
      "Epoch:  10  ===> Loss= 0.10678 , Training acurracy= 95.31%\n",
      "Epoch:  11  ===> Loss= 0.09950 , Training acurracy= 97.66%\n",
      "Epoch:  12  ===> Loss= 0.09228 , Training acurracy= 99.22%\n",
      "Epoch:  13  ===> Loss= 0.08592 , Training acurracy= 97.66%\n",
      "Epoch:  14  ===> Loss= 0.08196 , Training acurracy= 99.22%\n",
      "Epoch:  15  ===> Loss= 0.07795 , Training acurracy= 100.00%\n",
      "Epoch:  16  ===> Loss= 0.07332 , Training acurracy= 100.00%\n",
      "Epoch:  17  ===> Loss= 0.07046 , Training acurracy= 100.00%\n",
      "Epoch:  18  ===> Loss= 0.06750 , Training acurracy= 100.00%\n",
      "Epoch:  19  ===> Loss= 0.06435 , Training acurracy= 99.22%\n",
      "Epoch:  20  ===> Loss= 0.06205 , Training acurracy= 98.44%\n",
      "Epoch:  21  ===> Loss= 0.05960 , Training acurracy= 100.00%\n",
      "Epoch:  22  ===> Loss= 0.05723 , Training acurracy= 99.22%\n",
      "Epoch:  23  ===> Loss= 0.05537 , Training acurracy= 100.00%\n",
      "Epoch:  24  ===> Loss= 0.05352 , Training acurracy= 100.00%\n",
      "Epoch:  25  ===> Loss= 0.05145 , Training acurracy= 100.00%\n",
      "Epoch:  26  ===> Loss= 0.05008 , Training acurracy= 99.22%\n",
      "Epoch:  27  ===> Loss= 0.04875 , Training acurracy= 100.00%\n",
      "Epoch:  28  ===> Loss= 0.04683 , Training acurracy= 99.22%\n",
      "Epoch:  29  ===> Loss= 0.04558 , Training acurracy= 98.44%\n",
      "Epoch:  30  ===> Loss= 0.04446 , Training acurracy= 99.22%\n",
      "Epoch:  31  ===> Loss= 0.04303 , Training acurracy= 97.66%\n",
      "Epoch:  32  ===> Loss= 0.04183 , Training acurracy= 99.22%\n",
      "Epoch:  33  ===> Loss= 0.04071 , Training acurracy= 100.00%\n",
      "Epoch:  34  ===> Loss= 0.03944 , Training acurracy= 100.00%\n",
      "Epoch:  35  ===> Loss= 0.03860 , Training acurracy= 100.00%\n",
      "Epoch:  36  ===> Loss= 0.03768 , Training acurracy= 99.22%\n",
      "Epoch:  37  ===> Loss= 0.03641 , Training acurracy= 99.22%\n",
      "Epoch:  38  ===> Loss= 0.03552 , Training acurracy= 99.22%\n",
      "Epoch:  39  ===> Loss= 0.03473 , Training acurracy= 100.00%\n",
      "Epoch:  40  ===> Loss= 0.03368 , Training acurracy= 100.00%\n",
      "\n",
      "Time usage: 0:16:06\n",
      "\n",
      "Optimization Finished!\n",
      "\n",
      "\n",
      "Accuracy Test Set: 98.5% (9851 / 10000)\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    # Write logs to Tensorboard.\n",
    "    summary_writer = tf.summary.FileWriter('lab_2/log_files', graph=tf.get_default_graph())\n",
    "    # Run train process.\n",
    "    train(init, sess, training_epochs, batch_size, optimizer, cost, merged_summary_op)\n",
    "    print('\\nOptimization Finished!')\n",
    "    summary_writer.flush()\n",
    "    # Print test accurancy.\n",
    "    test_accuracy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Question 2.1.6 </b> : Use TensorBoard to visualise and save loss and accuracy curves. \n",
    "You will save figures in the folder **\"lab_2/MNIST_figures\"** and display them in your notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The respective images can be found in the folder. They are not displayed becuase we noticed an issue when we exported as html."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Part 2 </b> : LeNET 5 Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<b> Question 2.2.1 </b>\n",
    "\n",
    "- Retrain your network with AdamOptimizer and then fill the table above:\n",
    "\n",
    "\n",
    "| Optimizer            |  Gradient Descent  |    AdamOptimizer    |\n",
    "|----------------------|--------------------|---------------------|\n",
    "| Testing Accuracy     |       0.9850       |      0.7745         |       \n",
    "| Training Time        |       0:16:06      |      0:15:01        | \n",
    "\n",
    "- Which optimizer gives the best accuracy on test data?<br>\n",
    "According to the results, both methods give us similar results. In terms of accuracy, anyone gives similar results. In our opinion you cannot compare accuracy first because they start randomly at differents points and second because AdamOptimizer includes momentum in its algorithm to speed up calculation. The strage thing here is why both method reach similar end times. Probably becuase the load in the ZOE system does not allow it to compare correctly or maybe becayse AdamOptimizer needs to compute more things internatlly before to get the right momentum. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Input variables'''\n",
    "x = tf.placeholder(tf.float32, shape=[None, img_size_flat], name='images')\n",
    "y_true = tf.placeholder(tf.float32, shape=[None, num_classes], name='y_true')\n",
    "\n",
    "x_image = tf.reshape(x, [-1, img_size, img_size, num_channels])\n",
    "\n",
    "'''CNN Tensorflow architecture'''\n",
    "# Convolutional layer 2\n",
    "layer_conv1, weights_conv1 = LeNet5_Model(input=x_image,\n",
    "                                           num_input_channels=num_channels,\n",
    "                                           filter_size=filter_size_1,\n",
    "                                           num_filters=num_filters_1,\n",
    "                                           padding=padding_1,\n",
    "                                           activ_pooling=True)\n",
    "# Convolutional layer 2\n",
    "layer_conv2, weights_conv2 = LeNet5_Model(input=layer_conv1,\n",
    "                                           num_input_channels=num_filters_1,\n",
    "                                           filter_size=filter_size_2,\n",
    "                                           num_filters=num_filters_2,\n",
    "                                           padding=padding_2,\n",
    "                                           activ_pooling=True)\n",
    "\n",
    "# Flatten layer\n",
    "layer_flat, num_features = flatten_layer(layer_conv2)\n",
    "\n",
    "# Fully Connectes layer 1\n",
    "fc_layer_1 = new_fc_layer(input=layer_flat,\n",
    "                         num_inputs=num_features,\n",
    "                         num_outputs=fc_size_1)\n",
    "\n",
    "\n",
    "# Fully Connectes layer 2\n",
    "fc_layer_2 = new_fc_layer(input=fc_layer_1,\n",
    "                         num_inputs=fc_size_1,\n",
    "                         num_outputs=fc_size_2)\n",
    "\n",
    "\n",
    "# Fully Connectes layer 3\n",
    "fc_layer_3 = new_fc_layer(input=fc_layer_2,\n",
    "                         num_inputs=fc_size_2,\n",
    "                         num_outputs=num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction label from model (max value of Softmax operation).\n",
    "y_pred_cls = tf.argmax(tf.nn.softmax(fc_layer_3), axis=1)\n",
    "\n",
    "# Real label from data\n",
    "y_true_cls = tf.argmax(y_true, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = evaluate(y_pred_cls, y_true_cls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function.\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits_v2(logits=fc_layer_3,\n",
    "                                                        labels=y_true)\n",
    "# cost = tf.reduce_mean(cross_entropy)\n",
    "\n",
    "with tf.name_scope('Loss_Adam'):\n",
    "    cost = tf.reduce_mean(cross_entropy)\n",
    "\n",
    "with tf.name_scope('Accuracy_Adam'):\n",
    "    accuracy = evaluate(y_pred_cls, y_true_cls)\n",
    "\n",
    "# Gradient Descent optimizer.\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=0.01).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Create a summary to monitor cost tensor\n",
    "tf.summary.scalar('Cost_Adam', cost)\n",
    "\n",
    "# Create a summary to monitor accuracy tensor\n",
    "tf.summary.scalar('Accuracy_Adam', accuracy)\n",
    "\n",
    "# Merge all summaries into a single op\n",
    "merged_summary_op = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  01  ===> Loss= 1.63065 , Training acurracy= 36.72%\n",
      "Epoch:  02  ===> Loss= 1.41042 , Training acurracy= 39.84%\n",
      "Epoch:  03  ===> Loss= 1.40150 , Training acurracy= 41.41%\n",
      "Epoch:  04  ===> Loss= 1.29377 , Training acurracy= 52.34%\n",
      "Epoch:  05  ===> Loss= 1.17829 , Training acurracy= 56.25%\n",
      "Epoch:  06  ===> Loss= 1.16743 , Training acurracy= 50.78%\n",
      "Epoch:  07  ===> Loss= 1.17000 , Training acurracy= 56.25%\n",
      "Epoch:  08  ===> Loss= 1.16721 , Training acurracy= 46.88%\n",
      "Epoch:  09  ===> Loss= 1.16876 , Training acurracy= 43.75%\n",
      "Epoch:  10  ===> Loss= 1.16363 , Training acurracy= 55.47%\n",
      "Epoch:  11  ===> Loss= 1.16558 , Training acurracy= 58.59%\n",
      "Epoch:  12  ===> Loss= 1.16526 , Training acurracy= 53.12%\n",
      "Epoch:  13  ===> Loss= 1.14774 , Training acurracy= 62.50%\n",
      "Epoch:  14  ===> Loss= 0.95410 , Training acurracy= 69.53%\n",
      "Epoch:  15  ===> Loss= 0.68768 , Training acurracy= 71.09%\n",
      "Epoch:  16  ===> Loss= 0.50476 , Training acurracy= 76.56%\n",
      "Epoch:  17  ===> Loss= 0.49570 , Training acurracy= 76.56%\n",
      "Epoch:  18  ===> Loss= 0.48628 , Training acurracy= 78.91%\n",
      "Epoch:  19  ===> Loss= 0.48210 , Training acurracy= 78.12%\n",
      "Epoch:  20  ===> Loss= 0.48943 , Training acurracy= 81.25%\n",
      "Epoch:  21  ===> Loss= 0.47767 , Training acurracy= 83.59%\n",
      "Epoch:  22  ===> Loss= 0.48013 , Training acurracy= 85.16%\n",
      "Epoch:  23  ===> Loss= 0.47548 , Training acurracy= 85.94%\n",
      "Epoch:  24  ===> Loss= 0.48320 , Training acurracy= 86.72%\n",
      "Epoch:  25  ===> Loss= 0.47598 , Training acurracy= 79.69%\n",
      "Epoch:  26  ===> Loss= 0.49433 , Training acurracy= 82.81%\n",
      "Epoch:  27  ===> Loss= 0.48169 , Training acurracy= 77.34%\n",
      "Epoch:  28  ===> Loss= 0.48086 , Training acurracy= 75.00%\n",
      "Epoch:  29  ===> Loss= 0.47224 , Training acurracy= 71.88%\n",
      "Epoch:  30  ===> Loss= 0.48209 , Training acurracy= 78.12%\n",
      "Epoch:  31  ===> Loss= 0.47630 , Training acurracy= 82.81%\n",
      "Epoch:  32  ===> Loss= 0.47129 , Training acurracy= 82.03%\n",
      "Epoch:  33  ===> Loss= 0.47719 , Training acurracy= 84.38%\n",
      "Epoch:  34  ===> Loss= 0.49324 , Training acurracy= 77.34%\n",
      "Epoch:  35  ===> Loss= 0.50400 , Training acurracy= 84.38%\n",
      "Epoch:  36  ===> Loss= 0.49035 , Training acurracy= 81.25%\n",
      "Epoch:  37  ===> Loss= 0.47434 , Training acurracy= 89.06%\n",
      "Epoch:  38  ===> Loss= 0.49002 , Training acurracy= 79.69%\n",
      "Epoch:  39  ===> Loss= 0.49349 , Training acurracy= 89.84%\n",
      "Epoch:  40  ===> Loss= 0.48919 , Training acurracy= 75.78%\n",
      "\n",
      "Time usage: 0:15:01\n",
      "\n",
      "Optimization Finished!\n",
      "\n",
      "\n",
      "Accuracy Test Set: 77.5% (7745 / 10000)\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:    \n",
    "    sess.run(init)\n",
    "    # Write logs to Tensorboard.\n",
    "    summary_writer = tf.summary.FileWriter(logs_path, graph=tf.get_default_graph())\n",
    "    # Run train process.\n",
    "    train(init, sess, training_epochs, batch_size, optimizer, cost, merged_summary_op)\n",
    "    print('\\nOptimization Finished!')\n",
    "    summary_writer.flush()\n",
    "    # Print test accurancy.\n",
    "    test_accuracy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Question 2.2.2</b> Try to add dropout (keep_prob = 0.75) before the first fully connected layer. You will use tf.nn.dropout for that purpose. What accuracy do you achieve on testing data?\n",
    "\n",
    "**Accuracy achieved on testing data:** ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LeNet5_Model_Dropout(input,                # Input layer.\n",
    "                         num_input_channels,   # Channels in previous layer.\n",
    "                         filter_size,          # Width and height filter.\n",
    "                         num_filters,          # Number of filters.\n",
    "                         padding,              # Padding approach.\n",
    "                         activ_pooling=True):  # Use 2x2 max-pooling.\n",
    "\n",
    "    # Shape of the filter-weights.\n",
    "    shape = [filter_size, filter_size, num_input_channels, num_filters]\n",
    "\n",
    "    # Initialize weight given shape.\n",
    "    weights = weight_variable(shape=shape)\n",
    "\n",
    "    # Initialize bias given shape, one for each filter.\n",
    "    biases = bias_variable(shape=num_filters)\n",
    "\n",
    "    # Convolution operation.\n",
    "    layer = tf.nn.conv2d(input=input,\n",
    "                         filter=weights,\n",
    "                         strides=[1, 1, 1, 1],\n",
    "                         padding=padding)\n",
    "\n",
    "    # Add the biases to the results of the convolution.\n",
    "    # A bias-value is added to each filter-channel.\n",
    "    layer += biases\n",
    "\n",
    "    # Use pooling according to LeNet-5 architecture.\n",
    "    if activ_pooling:\n",
    "        # This is 2x2 max-pooling, which applies\n",
    "        # in all cases of CNN architecture.\n",
    "        layer = tf.nn.max_pool(value=layer,\n",
    "                               ksize=[1, 2, 2, 1],\n",
    "                               strides=[1, 2, 2, 1],\n",
    "                               padding='VALID')\n",
    "\n",
    "    # Layer dropout with keep_prob = 0.75\n",
    "    layer = tf.nn.dropout(layer, keep_prob = 0.75)\n",
    "    # Rectified Linear Unit (ReLU).\n",
    "    layer = tf.nn.relu(layer)\n",
    "\n",
    "    return layer, weights\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.01\n",
    "training_epochs = 40\n",
    "batch_size = 128\n",
    "logs_path = 'log_files/'\n",
    "\n",
    "'''Input variables'''\n",
    "x = tf.placeholder(tf.float32, shape=[None, img_size_flat], name='images')\n",
    "y_true = tf.placeholder(tf.float32, shape=[None, num_classes], name='y_true')\n",
    "\n",
    "x_image = tf.reshape(x, [-1, img_size, img_size, num_channels])\n",
    "\n",
    "'''CNN Tensorflow architecture'''\n",
    "# Convolutional layer 2\n",
    "layer_conv1, weights_conv1 = LeNet5_Model(input=x_image,\n",
    "                                           num_input_channels=num_channels,\n",
    "                                           filter_size=filter_size_1,\n",
    "                                           num_filters=num_filters_1,\n",
    "                                           padding=padding_1,\n",
    "                                           activ_pooling=True)\n",
    "# Convolutional layer 2\n",
    "layer_conv2, weights_conv2 = LeNet5_Model_Dropout(input=layer_conv1,\n",
    "                                           num_input_channels=num_filters_1,\n",
    "                                           filter_size=filter_size_2,\n",
    "                                           num_filters=num_filters_2,\n",
    "                                           padding=padding_2,\n",
    "                                           activ_pooling=True)\n",
    "\n",
    "# Flatten layer\n",
    "layer_flat, num_features = flatten_layer(layer_conv2)\n",
    "\n",
    "# Fully Connectes layer 1\n",
    "fc_layer_1 = new_fc_layer(input=layer_flat,\n",
    "                         num_inputs=num_features,\n",
    "                         num_outputs=fc_size_1)\n",
    "\n",
    "\n",
    "# Fully Connectes layer 2\n",
    "fc_layer_2 = new_fc_layer(input=fc_layer_1,\n",
    "                         num_inputs=fc_size_1,\n",
    "                         num_outputs=fc_size_2)\n",
    "\n",
    "\n",
    "# Fully Connectes layer 3\n",
    "fc_layer_3 = new_fc_layer(input=fc_layer_2,\n",
    "                         num_inputs=fc_size_2,\n",
    "                         num_outputs=num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction label from model (max value of Softmax operation).\n",
    "y_pred_cls = tf.argmax(tf.nn.softmax(fc_layer_3), axis=1)\n",
    "\n",
    "# Real label from data\n",
    "y_true_cls = tf.argmax(y_true, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = evaluate(y_pred_cls, y_true_cls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function.\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits_v2(logits=fc_layer_3,\n",
    "                                                        labels=y_true)\n",
    "# cost = tf.reduce_mean(cross_entropy)\n",
    "\n",
    "with tf.name_scope('Loss_Drop'):\n",
    "    cost = tf.reduce_mean(cross_entropy)\n",
    "\n",
    "with tf.name_scope('Accuracy_Drop'):\n",
    "    accuracy = evaluate(y_pred_cls, y_true_cls)\n",
    "\n",
    "# Gradient Descent optimizer.\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Create a summary to monitor cost tensor\n",
    "tf.summary.scalar('Cost_Drop', cost)\n",
    "\n",
    "# Create a summary to monitor accuracy tensor\n",
    "tf.summary.scalar('Accuracy_Drop', accuracy)\n",
    "\n",
    "# Merge all summaries into a single op\n",
    "merged_summary_op = tf.summary.merge_all()\n",
    "\n",
    "# Initializing the variables\n",
    "no_iter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  01  ===> Loss= 1.93373 , Training acurracy= 57.81%\n",
      "Epoch:  02  ===> Loss= 0.86281 , Training acurracy= 74.22%\n",
      "Epoch:  03  ===> Loss= 0.63920 , Training acurracy= 85.16%\n",
      "Epoch:  04  ===> Loss= 0.54896 , Training acurracy= 87.50%\n",
      "Epoch:  05  ===> Loss= 0.49489 , Training acurracy= 86.72%\n",
      "Epoch:  06  ===> Loss= 0.46134 , Training acurracy= 92.19%\n",
      "Epoch:  07  ===> Loss= 0.43528 , Training acurracy= 85.94%\n",
      "Epoch:  08  ===> Loss= 0.41449 , Training acurracy= 84.38%\n",
      "Epoch:  09  ===> Loss= 0.39937 , Training acurracy= 90.62%\n",
      "Epoch:  10  ===> Loss= 0.38671 , Training acurracy= 86.72%\n",
      "Epoch:  11  ===> Loss= 0.37641 , Training acurracy= 89.84%\n",
      "Epoch:  12  ===> Loss= 0.36817 , Training acurracy= 91.41%\n",
      "Epoch:  13  ===> Loss= 0.35923 , Training acurracy= 92.19%\n",
      "Epoch:  14  ===> Loss= 0.35567 , Training acurracy= 87.50%\n",
      "Epoch:  15  ===> Loss= 0.34811 , Training acurracy= 89.84%\n",
      "Epoch:  16  ===> Loss= 0.34364 , Training acurracy= 90.62%\n",
      "Epoch:  17  ===> Loss= 0.34052 , Training acurracy= 87.50%\n",
      "Epoch:  18  ===> Loss= 0.33564 , Training acurracy= 88.28%\n",
      "Epoch:  19  ===> Loss= 0.33331 , Training acurracy= 89.06%\n",
      "Epoch:  20  ===> Loss= 0.32949 , Training acurracy= 89.06%\n",
      "Epoch:  21  ===> Loss= 0.32736 , Training acurracy= 85.94%\n",
      "Epoch:  22  ===> Loss= 0.32330 , Training acurracy= 82.81%\n",
      "Epoch:  23  ===> Loss= 0.32207 , Training acurracy= 88.28%\n",
      "Epoch:  24  ===> Loss= 0.31947 , Training acurracy= 90.62%\n",
      "Epoch:  25  ===> Loss= 0.31627 , Training acurracy= 89.06%\n",
      "Epoch:  26  ===> Loss= 0.31565 , Training acurracy= 87.50%\n",
      "Epoch:  27  ===> Loss= 0.31155 , Training acurracy= 85.16%\n",
      "Epoch:  28  ===> Loss= 0.31143 , Training acurracy= 87.50%\n",
      "Epoch:  29  ===> Loss= 0.30947 , Training acurracy= 88.28%\n",
      "Epoch:  30  ===> Loss= 0.30830 , Training acurracy= 92.19%\n",
      "Epoch:  31  ===> Loss= 0.30517 , Training acurracy= 91.41%\n",
      "Epoch:  32  ===> Loss= 0.30511 , Training acurracy= 89.06%\n",
      "Epoch:  33  ===> Loss= 0.30346 , Training acurracy= 89.06%\n",
      "Epoch:  34  ===> Loss= 0.30145 , Training acurracy= 87.50%\n",
      "Epoch:  35  ===> Loss= 0.30006 , Training acurracy= 90.62%\n",
      "Epoch:  36  ===> Loss= 0.30018 , Training acurracy= 89.06%\n",
      "Epoch:  37  ===> Loss= 0.29828 , Training acurracy= 89.06%\n",
      "Epoch:  38  ===> Loss= 0.29579 , Training acurracy= 86.72%\n",
      "Epoch:  39  ===> Loss= 0.29759 , Training acurracy= 85.94%\n",
      "Epoch:  40  ===> Loss= 0.29509 , Training acurracy= 90.62%\n",
      "\n",
      "Time usage: 0:15:22\n",
      "\n",
      "Optimization Finished!\n",
      "\n",
      "\n",
      "Accuracy Test Set: 88.7% (8871 / 10000)\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    # Write logs to Tensorboard.\n",
    "    summary_writer = tf.summary.FileWriter(logs_path, graph=tf.get_default_graph())\n",
    "    # Run train process.\n",
    "    train(init, sess, training_epochs, batch_size, optimizer, cost, merged_summary_op)\n",
    "    print('\\nOptimization Finished!')\n",
    "    summary_writer.flush()\n",
    "    # Print test accurancy.\n",
    "    test_accuracy()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
